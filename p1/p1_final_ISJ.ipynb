{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add total spendings column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) FRESH: annual spending (m.u.) on fresh products (Continuous);\n",
    "# 2) MILK: annual spending (m.u.) on milk products (Continuous);\n",
    "# 3) GROCERY: annual spending (m.u.) on grocery products (Continuous);\n",
    "# 4) FROZEN: annual spending (m.u.) on frozen products (Continuous)\n",
    "# 5) DETERGENTS_PAPER: annual spending (m.u.) on detergents and paper products (Continuous)\n",
    "# 6) DELICATESSEN: annual spending (m.u.) on and delicatessen products (Continuous);\n",
    "# 7) CHANNEL: customers' Channel - Horeca (Hotel/Restaurant/CafÃ©) or Retail channel (Nominal)\n",
    "# 8) REGION: customers' Region - Lisnon, Oporto or Other (Nominal)\n",
    "# Descriptive Statistics:\n",
    "\n",
    "# (Minimum, Maximum, Mean, Std. Deviation)\n",
    "# FRESH ( 3, 112151, 12000.30, 12647.329)\n",
    "# MILK (55, 73498, 5796.27, 7380.377)\n",
    "# GROCERY (3, 92780, 7951.28, 9503.163)\n",
    "# FROZEN (25, 60869, 3071.93, 4854.673)\n",
    "# DETERGENTS_PAPER (3, 40827, 2881.49, 4767.854)\n",
    "# DELICATESSEN (3, 47943, 1524.87, 2820.106)\n",
    "\n",
    "# REGION Frequency\n",
    "# Lisbon 77\n",
    "# Oporto 47\n",
    "# Other Region 316\n",
    "# Total 440\n",
    "\n",
    "# CHANNEL Frequency\n",
    "# Horeca 298\n",
    "# Retail 142\n",
    "# Total 440\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1 Loading the Data, Preprocessing, Initial Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv\")\n",
    "df = df.drop(columns=[\"Channel\", \"Region\"])\n",
    "df_og = df.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_cols():\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(df.columns):\n",
    "        sns.histplot(df[col], ax=axes[i])\n",
    "\n",
    "\n",
    "plot_hist_cols()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.log(df + 1)\n",
    "df_overview = df.copy()\n",
    "plot_hist_cols()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)\n",
    "# TODO: Add Correlation Plot and a column with total spendings\n",
    "# Barplot/Pie chart with spendings from total spendings\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Detecting Anomalies\n",
    "## Hard-Min Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm=\"ball_tree\").fit(df)\n",
    "distances, indices = nbrs.kneighbors(df)\n",
    "df_overview[\"outlier_score_min\"] = np.square(distances[:, 1])\n",
    "df_overview.sort_values(by=\"outlier_score_min\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distances.shape)\n",
    "# distances: Zeile = sample, Spalte 1 = kürzeste Distanz\n",
    "# Spalte 0 = alles 0\n",
    "print(indices.shape)\n",
    "# indices: erste Spalte 0-440; zweite Spalte: index zur kürzesten Distanz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distances[0, 1], indices[0])\n",
    "# quick double check\n",
    "np.linalg.norm(df.iloc[0, :] - df.iloc[59, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.var(df_overview['outlier_score_min']))\n",
    "display(df_overview[[\"outlier_score_min\"]].describe())\n",
    "sns.boxplot(df_overview[\"outlier_score_min\"])\n",
    "plt.show()\n",
    "sns.histplot(df_overview[\"outlier_score_min\"])\n",
    "# want it to be centered/dense!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft-Min Score/KDE approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=len(df), algorithm=\"ball_tree\").fit(df)\n",
    "distances, indices = nbrs.kneighbors(df)\n",
    "distances = np.square(distances)\n",
    "gamma = 1\n",
    "\n",
    "def softmin(z, gamma):\n",
    "    # return -1 / gamma * np.log(1 / (len(z) - 1) * np.sum(np.exp(-gamma * z)))\n",
    "    return -1 * np.log(1 / (len(z) - 1) * np.sum(np.exp(-gamma * z)))\n",
    "\n",
    "\n",
    "df_overview[\"outlier_score_softmin\"] = np.apply_along_axis(softmin, 1, distances[:, 1:], gamma)\n",
    "df_overview.sort_values(by=\"outlier_score_softmin\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distances.shape)\n",
    "\n",
    "print(np.sum(distances == 0))\n",
    "distances # aufsteigende Reihenfolge, indices enthält die zugehörigen Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Nebeneinander packe\n",
    "\n",
    "print(np.var(df_overview['outlier_score_softmin']))\n",
    "display(df_overview[[\"outlier_score_softmin\"]].describe())\n",
    "sns.boxplot(df_overview[\"outlier_score_softmin\"])\n",
    "plt.show()\n",
    "sns.histplot(df_overview[\"outlier_score_softmin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize softmin function for a selected instance\n",
    "instance = 338  # 338 has had a high outlier score\n",
    "gammas = np.linspace(0.1, 2, 100)\n",
    "g = sns.lineplot(\n",
    "    x=gammas, y=[softmin(distances[instance, 1:], gamma) for gamma in gammas]\n",
    ")\n",
    "g.set_xlabel(\"gamma\")\n",
    "g.set_ylabel(\"softmin\") \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap/Robustness estimate & choosing bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO should that be the same?\n",
    "# import statistics\n",
    "# statistics.fmean(np.exp(-gamma * distances[0, 1:]))\n",
    "# softmin(distances[instance, 1:], gamma)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hohes gamma => weniger punkte einbezogen => höhere varianz, weniger bias\n",
    "\n",
    "niedrigeres gamma => mehr punkte einbezogen (lim => 0 all uniform) => hoher bias, low variance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need 2 measures to compare: robustness vs. value of score (how much it discriminates between outliers and \"regular\" points); check picture from Montavon. Visual examination/optimal point. Compare to hard min\n",
    "\n",
    "\n",
    "Cluster = Partition der Daten um sie zusammenzufassen, nicht unbedingt obvious"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUGGESTED APPROACH ON TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmin_og(z, gamma):\n",
    "    return - (1/gamma)  * np.log(1 / (len(z) - 1) * np.sum(np.exp(-gamma * z)))\n",
    "\n",
    "# NECESSARY TO USE ALL INSTANCES\n",
    "# def clean_distances(z):\n",
    "#     if z[0] == 0:\n",
    "#         return z[1:]\n",
    "#     else:\n",
    "#         return z[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BOOTSTRAP = 100\n",
    "gamma_range = np.linspace(0.1, 2, 20)\n",
    "\n",
    "scores = np.full((len(df), N_BOOTSTRAP, len(gamma_range)), np.nan)\n",
    "\n",
    "\n",
    "for i in range(0, N_BOOTSTRAP):\n",
    "\n",
    "    sample = df.sample(frac=0.5)\n",
    "\n",
    "    # USE ALL INSTANCES\n",
    "    # nbrs = NearestNeighbors(n_neighbors=len(sample), algorithm=\"ball_tree\").fit(sample)\n",
    "    # distances, indices = nbrs.kneighbors(df)                                                    \n",
    "    # distances = np.apply_along_axis(clean_distances, 1, distances)\n",
    "    # distances = np.square(distances)\n",
    "    \n",
    "    # for j, gamma in enumerate(gamma_range):\n",
    "    #     scores[:, i, j] = np.apply_along_axis(softmin_og, 1, distances, gamma)\n",
    "\n",
    "\n",
    "    # TODO all instanes or only those in sample???\n",
    "    # USE ONLY IN SAMPLE\n",
    "    nbrs = NearestNeighbors(n_neighbors=len(sample), algorithm=\"ball_tree\").fit(sample)\n",
    "    distances, indices = nbrs.kneighbors(sample)\n",
    "    distances = np.square(distances[:, 1:])\n",
    "    \n",
    "    for j, gamma in enumerate(gamma_range):\n",
    "        scores[sample.index, i, j] = np.apply_along_axis(softmin_og, 1, distances, gamma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check different when using all vs. only in sample\n",
    "# spread = np.var(scores, axis=1)\n",
    "# spread_only = np.nanvar(scores_only, axis=1)\n",
    "# display(pd.DataFrame(np.linalg.norm(spread - spread_only, axis=1)).describe())\n",
    "\n",
    "# verify normal distribution arround N_BOOTSTRAP * frac\n",
    "sns.histplot(np.sum(np.isnan(scores) == False, axis=1)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance 0, first bootstrap sample, all values for gamma\n",
    "display(scores[0, 0])\n",
    "\n",
    "# first bootstrap sample, all instances per gamma\n",
    "for i in range(len(scores)):\n",
    "    plt.plot(gamma_range, scores[i, 0], linewidth=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the spread over bootstrap dimension\n",
    "spread = np.nanvar(scores, axis=1)\n",
    "\n",
    "display(spread.shape)\n",
    "print(\"Vars of instance 0:\")\n",
    "display(spread[0])\n",
    "\n",
    "print(\"Vars of instance 338:\")\n",
    "display(spread[338])\n",
    "\n",
    "# high values (variances) => anomaly score varied more in bootstrap samples => outlier scoring of instance is not so robust (it varies with sample)\n",
    "\n",
    "# we want detection to be robust => so to have a low variance during bootstrap experiments\n",
    "\n",
    "# all spreads per instances\n",
    "for i in range(len(scores)):\n",
    "    plt.plot(gamma_range, spread[i], linewidth=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma high => less points taken into account => on average higher variance (and less robust, right?)\n",
    "g = sns.scatterplot(x=gamma_range, y=np.mean(np.nanvar(scores, axis=1), axis=0))\n",
    "g.set_xlabel('gamma')\n",
    "g.set_ylabel('Mean(Var(scores))')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average outlier score per sample, per gamma\n",
    "avg_score = np.nanmean(scores, axis=1)\n",
    "\n",
    "# TODO are the actual score values event meaningful? the range seems to depend on gamma \n",
    "# maybe the resulting ranking is more interesting?\n",
    "\n",
    "\n",
    "display(avg_score.shape)\n",
    "print(\"average outlier score of instance 0:\")\n",
    "display(avg_score[0])\n",
    "\n",
    "print(\"average outlier score of instance 338:\")\n",
    "display(avg_score[338])\n",
    "\n",
    "# high gamma => less values are taken into account => TODO seems to result in lower average outlier score (due to influence of gamma) but why?\n",
    "\n",
    "# all avg_scores per instances\n",
    "for i in range(len(scores)):\n",
    "    plt.plot(gamma_range, avg_score[i], linewidth=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.scatterplot(x=gamma_range, y=np.var(np.nanmean(scores, axis=1), axis=0))\n",
    "g.set_xlabel('gamma')\n",
    "g.set_ylabel('Var(Mean(scores))')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.scatterplot(x=np.mean(np.nanvar(scores, axis=1), axis=0), y=np.var(np.nanmean(scores, axis=1), axis=0), hue=gamma_range)\n",
    "g.set_xlabel('Mean(Var(scores))')\n",
    "g.set_ylabel('Var(Mean(scores))')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO hard-min adden\n",
    "# TODO ask about 1/gamma in softmin \n",
    "\n",
    "# var(mean(scores)) == discriminatory power ?\n",
    "# mean(var(scores)) == model var?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIFFERENT APPROACH: SCORE DIFFERENCE IN 50/50 SPLIT\n",
    "Does that solve the range problems caused by gamma?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BOOTSTRAP = 100\n",
    "gamma_range = np.linspace(0.1, 2, 40)\n",
    "\n",
    "score_diff = np.zeros((len(df), N_BOOTSTRAP, len(gamma_range)))\n",
    "\n",
    "hardmin_diff = np.zeros((len(df), N_BOOTSTRAP))\n",
    "\n",
    "# NECESSARY TO USE ALL INSTANCES\n",
    "def clean_distances(z):\n",
    "    if z[0] == 0:\n",
    "        return z[1:]\n",
    "    else:\n",
    "        return z[0:-1]\n",
    "\n",
    "\n",
    "for i in range(0, N_BOOTSTRAP):\n",
    "\n",
    "    # 50/50 split\n",
    "    sample = df.sample(frac=0.5)\n",
    "    rest = df.drop(sample.index)\n",
    "\n",
    "    # USE ALL INSTANCES\n",
    "    nbrs = NearestNeighbors(n_neighbors=len(sample), algorithm=\"ball_tree\").fit(sample)\n",
    "    distances1, _ = nbrs.kneighbors(df)\n",
    "    distances1 = np.apply_along_axis(clean_distances, 1, distances1)\n",
    "    distances1 = np.square(distances1)\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=len(rest), algorithm=\"ball_tree\").fit(rest)\n",
    "    distances2, _ = nbrs.kneighbors(df)\n",
    "    distances2 = np.apply_along_axis(clean_distances, 1, distances2)\n",
    "    distances2 = np.square(distances2)\n",
    "\n",
    "    for j, gamma in enumerate(gamma_range):\n",
    "        sm1 = np.apply_along_axis(softmin_og, 1, distances1, gamma)\n",
    "        sm2 = np.apply_along_axis(softmin_og, 1, distances2, gamma)\n",
    "\n",
    "        score_diff[:, i, j] = sm2 - sm1\n",
    "\n",
    "    # hardmin\n",
    "    hardmin1 = distances1[:, 1]\n",
    "    hardmin2 = distances2[:, 1]\n",
    "\n",
    "    hardmin_diff[:, i] = hardmin2 - hardmin1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hardmin has low bias but high variance (not so robust)\n",
    "# TODO interpretation of that value corect?\n",
    "np.mean(np.var(hardmin_diff, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hardmin score # TODO interpretation of that value?\n",
    "np.var(np.mean(hardmin_diff, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_diff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spread = np.var(score_diff, axis=1)\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    plt.plot(gamma_range, spread[i], linewidth=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# robustness (how much to the scores vary over the bootstrap samples?)\n",
    "# robustness increases with lower gamma (but high bias)\n",
    "sns.scatterplot(x = gamma_range, y=np.mean(spread, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score_diff = np.mean(score_diff, axis=1)\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    plt.plot(gamma_range, avg_score_diff[i], linewidth=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure discrimination \n",
    "# how good can we distinguish between the scores?\n",
    "# higher variance is better here to identify the outliers\n",
    "sns.scatterplot(x = gamma_range, y=np.var(avg_score_diff, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = np.mean(spread, axis=0), y=np.var(avg_score_diff, axis=0), hue=gamma_range)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Explaining Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance(x, yj, gamma):\n",
    "    \"\"\"Calculate layer-wise relevance propagation\n",
    "    @x: array of instances\n",
    "    @yj: softmin scores of all j instances\n",
    "    @gamma: gamma used for softmin \n",
    "    \"\"\"\n",
    "\n",
    "    Rji = np.zeros(x.shape)\n",
    "\n",
    "    # calculation per instance\n",
    "    for j in range(len(x)):\n",
    "\n",
    "        # mask is used to exclude the current instance j\n",
    "        mask = np.full((len(x)), True)\n",
    "        mask[j] = False\n",
    "\n",
    "        # save xk - xj\n",
    "        xk_j = x - x[j]\n",
    "\n",
    "        # calculate zk = ||xj - xk||^2\n",
    "        zk = np.square(np.linalg.norm(xk_j, axis=1))\n",
    "\n",
    "        # 1. First, one identifies to what extent each data point has contributed to the anomaly score of instance j\n",
    "        temp = np.exp(-gamma * zk[mask])\n",
    "        Rk = temp / np.sum(temp) * yj[j]\n",
    "\n",
    "        # 2. Then, these scores can be propagated back to the input features by observing that the (squared)\n",
    "        # Euclidean distance entering the anomaly score can be decomposed in terms of individual components:\n",
    "        Rji[j, :] = np.sum(np.square(xk_j)[mask] / zk[mask][:, None] * Rk[:, None], axis=0)\n",
    "\n",
    "    return Rji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1\n",
    "\n",
    "# Calculate anomaly scores\n",
    "nbrs = NearestNeighbors(n_neighbors=len(df), algorithm=\"ball_tree\").fit(df)\n",
    "distances, _ = nbrs.kneighbors(df)\n",
    "yj = np.apply_along_axis(softmin_og, 1, np.square(distances[:, 1:]), gamma)\n",
    "\n",
    "x = df.to_numpy()\n",
    "Rji = relevance(x, yj, gamma)\n",
    "\n",
    "# confirm conservation property\n",
    "np.all(np.sum(Rji, axis=1) - yj <= 1e-14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto', init='pca', perplexity=30).fit_transform(df)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10,8))\n",
    "points = ax.scatter(x=X_embedded[:,1], y=X_embedded[:,0], c=yj, s=30, cmap=plt.cm.get_cmap('viridis_r'))\n",
    "f.colorbar(points)\n",
    "ax.set_ylabel(\"t-SNE 1\")\n",
    "ax.set_xlabel(\"t-SNE 2\")\n",
    "ax.set_title(\"Outlier score in 2D projection (t-SNE) \")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = 338\n",
    "g = sns.barplot(x=Rji[instance], y=list(map(str, list(zip(df_og.iloc[instance].index, df_og.iloc[instance].values)))))\n",
    "g.set_xlim(0,np.max(Rji))\n",
    "plt.show()\n",
    "\n",
    "# TODO WHY IS Fresh = 3 not relevant for outlier score?\n",
    "print(\"Ri:\", Rji[instance])\n",
    "print(\"Outlier score:\", yj[instance])\n",
    "display(df.iloc[instance])\n",
    "display(df.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment to use explanations of these outlier scores for the reproducibility experiment in task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50% random sample without replacement\n",
    "# for each gamma:\n",
    "#   calculate anomaly scores for sample instances\n",
    "#   calculate relevance \n",
    "#   save relevance (per component) for later analysis\n",
    "\n",
    "gamma = 1\n",
    "\n",
    "N_BOOTSTRAP = 100\n",
    "\n",
    "Rji_BS = np.full((440, N_BOOTSTRAP, len(gamma_range), 6), np.nan)\n",
    "\n",
    "for i in range(N_BOOTSTRAP):\n",
    "\n",
    "    sample = df.sample(frac=0.5)\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=len(sample), algorithm=\"ball_tree\").fit(sample)\n",
    "    distances, _ = nbrs.kneighbors(sample)\n",
    "    distances = np.square(distances[:, 1:])\n",
    "\n",
    "    for k, g in enumerate(gamma_range):\n",
    "        yj = np.apply_along_axis(softmin_og, 1, distances, g)\n",
    "        Rji_BS[sample.index, i, k, :] = relevance(sample.to_numpy(), yj, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each instance, for each sample, for each gamma, the relevance per component\n",
    "Rji_BS.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment for all components at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse the mean Euclidean distance between the relevance components over all bootstrap samples (per gamma and per instance)\n",
    "# since the range of the anomaly scores differes with gamma, the range of the relevance components will also depend on gamma\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "mean_dist = np.zeros((len(Rji_BS), len(gamma_range)))\n",
    "var_dist = np.zeros((len(Rji_BS), len(gamma_range)))\n",
    "\n",
    "# iterate over instances\n",
    "for j in range(len(Rji_BS)):\n",
    "    # iterate over gammas\n",
    "    for k in range(len(gamma_range)):\n",
    "        mean_dist[j,k] = np.nanmean(distance.pdist(Rji_BS[j,:,k,:]))\n",
    "        var_dist[j,k] = np.nanvar(distance.pdist(Rji_BS[j,:,k,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axs = axs.flatten()\n",
    "\n",
    "\n",
    "for i in range(len(var_dist)):\n",
    "    axs[0].plot(gamma_range, var_dist[i], linewidth=0.3)\n",
    "\n",
    "sns.scatterplot(x=gamma_range, y=np.mean(var_dist, axis=0), ax=axs[2])\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(mean_dist)):\n",
    "    axs[1].plot(gamma_range, mean_dist[i], linewidth=0.3)\n",
    "\n",
    "sns.scatterplot(x=gamma_range, y=np.var(mean_dist, axis=0), ax=axs[3])\n",
    "\n",
    "\n",
    "axs[0].set_ylabel(\"instance variance of component distance\")\n",
    "axs[1].set_ylabel(\"instance mean of component distance\")\n",
    "axs[2].set_ylabel(\"mean of instance variance of component distance\")\n",
    "axs[3].set_ylabel(\"variance of instance mean of component distance\")\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"gamma\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment PER COMPONENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component = 0\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axs = axs.flatten()\n",
    "\n",
    "\n",
    "spread = np.nanvar(Rji_BS[:, :, :, component], axis=1)\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    axs[0].plot(gamma_range, spread[i], linewidth=0.3)\n",
    "\n",
    "sns.scatterplot(x=gamma_range, y=np.mean(spread, axis=0), ax=axs[2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "avg_rel = np.nanmean(Rji_BS[:, :, :, component], axis=1)\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    axs[1].plot(gamma_range, avg_rel[i], linewidth=0.3)\n",
    "\n",
    "sns.scatterplot(x=gamma_range, y=np.var(avg_rel, axis=0), ax=axs[3])\n",
    "\n",
    "\n",
    "axs[0].set_ylabel(\"instance variance of relevance\")\n",
    "axs[1].set_ylabel(\"instance mean of relevance\")\n",
    "axs[2].set_ylabel(\"mean of instance variance of relevance\")\n",
    "axs[3].set_ylabel(\"variance of instance mean of relevance\")\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"gamma\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO do these findings match with our other findings?\n",
    "# TODO the components behave differently - why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Cluster Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-2.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
