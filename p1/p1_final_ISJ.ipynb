{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.utils import resample\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "import p1_functions\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Loading the Data, Preprocessing, Initial Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv\")\n",
    "df_og = df.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the typo\n",
    "df.rename(columns = {'Delicassen':'Delicatessen'}, inplace = True)\n",
    "\n",
    "#Describe the dataframe\n",
    "df.describe()\n",
    "\n",
    "# Number of distinct elements per column\n",
    "#df.nunique()\n",
    "\n",
    "# Check for NaNs\n",
    "#df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Attribute Information:\n",
    "CHANNEL: customers' Channel - Horeca (Hotel/Restaurant/Cafe) or Retail channel (Nominal)\n",
    "REGION: customers' Region - Lisnon, Oporto or Other (Nominal)\n",
    "FRESH: annual spending (m.u.) on fresh products (Continuous)\n",
    "MILK: annual spending (m.u.) on milk products (Continuous)\n",
    "GROCERY: annual spending (m.u.)on grocery products (Continuous)\n",
    "FROZEN: annual spending (m.u.)on frozen products (Continuous)\n",
    "DETERGENTS_PAPER: annual spending (m.u.) on detergents and paper products (Continuous)\n",
    "DELICATESSEN: annual spending (m.u.)on and delicatessen products (Continuous)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get nominal and numerical attributes\n",
    "dtype = df.dtypes\n",
    "cat_features = [\"Channel\", \"Region\"] #df[[\"Channel\", \"Region\"]]\n",
    "num_features = df.drop(columns=[\"Channel\", \"Region\"]).columns.tolist()\n",
    "\n",
    "print(\"Nominal features:\", cat_features)\n",
    "print(\"Numerical features:\", num_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic statistical visualisations of the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot distributions of attributes: NOMINAL (barplots)\n",
    "p1_functions.cat_plot(df, cat_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot overall spending per category\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "df[num_features].sum().plot.bar()\n",
    "plt.title(\"Spending per Category\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot distributions of attributes: NUMERICAL (histograms, boxplot)\n",
    "# Plot correlations (pairwise scatterplots and heatmap)\n",
    "p1_functions.num_plot(df[num_features])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Observation: The distributions are heavy tailed -> apply the log function to the continuous features so that the distribution becomes compressed for large values and expanded for small values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop meta-data: Channel and the Region indicators\n",
    "df = df.drop(columns=cat_features)\n",
    "\n",
    "# Save the original dat set without meta-data\n",
    "df_og = df.copy()\n",
    "df_overview = df.copy() # we will use this to append the scores for an overview. Calculations are performed on df\n",
    "\n",
    "# x ‚Üê log(x + 1)\n",
    "df = np.log(df + 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recomputed plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Recompute plots after the transformation\n",
    "p1_functions.num_plot(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Observation: The log transformation produces more normally distributed attributes, however the statistics can not be reliably computed (e. g. correlations). Also, some lower values can now be considered extreme, while very few higher values still possess this quality (see boxplots)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Detecting Anomalies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard-Min Score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the distance to the nearest neighbor for each instance. This measure is used as an outlier score. We assume that this measure is not very biased, but has a high variance. We therefore assume that using a bootstrap with the Hard-Min approach produces a more accurate and robust outlierness score. Later on, (together with some visual inspections) we evalute the accuracy of a more robust, but more biased estimator of the scores. Note that we are interested in the resulting ranking of the scores and not the scores per se.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake it till you make it: Creating an artificial \"Ground Truth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the HYPERPARAMETERS\n",
    "N_BOOTSTRAP = 1000\n",
    "SAMPLE_SIZE_FRAC  = 0.5\n",
    "OUTLIERS_FRAC = 0.04"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spearman correlation as well as the scores increase with a higher sample size fraction (nearest neighbor distances increase monotonically since we consider fewer data points). To assess the bias of the soft Estimator, which we will consider later for different values of gamma, we classify the \"OUTLIERS_FRAC\" % of the most likely outliers from the hard-min bootstrap and check the accuracy.\n",
    "\n",
    "The Sample size fraction is chosen in a way s.t. the spearman correlation with the Hard-Min on the whole dataset is high, but not too high.\n",
    "\n",
    "The outliers frac of 4 % results in 14 samples from 440 in the dataset which seems a reasonable guess. This parameter can be adjusted given further domain knowledge or if an indicator is needed to make the outlier scores even more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Samples for Bootstrap approach\n",
    "samples = [resample(df, n_samples=math.ceil(len(df) * SAMPLE_SIZE_FRAC), replace=False) for i in range(N_BOOTSTRAP)]\n",
    "\n",
    "def hardmin_score(df):\n",
    "    nbrs = NearestNeighbors(n_neighbors=2, algorithm=\"ball_tree\").fit(df)\n",
    "    distances, indices = nbrs.kneighbors(df)\n",
    "    outlier_score_min = np.square(distances[:, 1])\n",
    "    return outlier_score_min\n",
    "\n",
    "\n",
    "def hardmin_bootstrap(n_bootstrap=N_BOOTSTRAP, sample_size_frac=SAMPLE_SIZE_FRAC, data=df, samples = samples):\n",
    "        hard_min_scores_bootstrap = pd.DataFrame({i: np.full(N_BOOTSTRAP, np.nan) for i in range(len(df))})\n",
    "        #samples = [resample(df, n_samples=math.ceil(len(df) * SAMPLE_SIZE_FRAC), replace=False) for i in range(N_BOOTSTRAP)]\n",
    "        for i in range(0, N_BOOTSTRAP):\n",
    "                sample = samples[i]\n",
    "                hard_min_scores_bootstrap.loc[i, sample.index] = hardmin_score(sample)\n",
    "                \n",
    "        return hard_min_scores_bootstrap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example for the bootstrap values table below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_min_scores_bootstrap = hardmin_bootstrap(df)\n",
    "display(hard_min_scores_bootstrap)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the Hard-Min outlier scores for the whole dataset as well as the mean outlier scores over the bootstrap samples without replacement. We than compare the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overview[\"hardmin_score\"] = hardmin_score(df)\n",
    "df_overview[\"hardmin_bootstrap_score\"] = hardmin_bootstrap(data=df).mean()\n",
    "#display(df_overview.head())\n",
    "df_overview.sort_values(by=\"hardmin_bootstrap_score\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check difference! Stabilized it a little bit\n",
    "sns.boxplot(np.abs(df_overview[\"hardmin_bootstrap_score\"] - df_overview[\"hardmin_score\"])).set_title(\"Difference between hardmin on whole data set vs. mean on bootstrap\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything gets down to choosing the right evaluation metric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Hard-Min and Hard-Min-bootstrap appraoches by statistical means as well as \"artificial\" outlier classification.\n",
    "\n",
    "Later when we will try to choose an optimal gamma, we will use the same approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_bias_eval(scores, baseline=df_overview[\"hardmin_bootstrap_score\"].to_numpy(), outlier_frac=OUTLIERS_FRAC,\n",
    "                    return_p_val=False):\n",
    "    n_outliers = math.ceil(len(baseline) * outlier_frac)\n",
    "    spr = spearmanr(scores, baseline)\n",
    "    hardmin_bootstrap_ranking = np.argsort(baseline * (-1))\n",
    "    hardmin_outliers = hardmin_bootstrap_ranking[:n_outliers]\n",
    "    score_outlier_ranking = np.argsort(scores * (-1))\n",
    "    score_outliers = score_outlier_ranking[:n_outliers]\n",
    "\n",
    "    accuracy = len(set(hardmin_outliers).intersection(score_outliers)) / n_outliers\n",
    "\n",
    "    return spr[0], accuracy\n",
    "\n",
    "spr, acc = score_bias_eval(scores=df_overview[\"hardmin_score\"])\n",
    "\n",
    "print(f\"Bootstrap Accuracy: {acc:.2}% \\nSpearman results: {spr}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Soft-Min Score/KDE approach"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better understanding what we'll do next, we will compute the soft-min scores for a fixed Gamma value and compare them to our baseline/ground-truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=len(df), algorithm=\"ball_tree\").fit(df)\n",
    "distances, indices = nbrs.kneighbors(df)\n",
    "distances = np.square(distances)\n",
    "\n",
    "def softmin(z, gamma=GAMMA):\n",
    "    return (-1 / gamma) * np.log( (1 / (len(z) - 1)) * np.sum(np.exp(-gamma * z)) )\n",
    "\n",
    "\n",
    "df_overview[\"outlier_score_softmin\"] = np.apply_along_axis(softmin, 1, distances[:, 1:], GAMMA)\n",
    "spr, acc = score_bias_eval(scores=df_overview[\"outlier_score_softmin\"])\n",
    "\n",
    "print(f\"Bootstrap Accuracy: {acc:.2}% \\nSpearman results: {spr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually compare distributions of the scores\n",
    "fig, axs = plt.subplots(2,2, figsize=(10,10))\n",
    "\n",
    "sns.boxplot(df_overview[\"outlier_score_softmin\"], ax=axs[0, 0])\n",
    "sns.histplot(df_overview[\"outlier_score_softmin\"], ax=axs[1, 0])\n",
    "sns.boxplot(df_overview[\"hardmin_bootstrap_score\"], ax=axs[0, 1])\n",
    "sns.histplot(df_overview[\"hardmin_bootstrap_score\"], ax=axs[1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distributions are quite similar although they \"operate\" on different scales. When we thought about how to evaluate different measures, first we tried to use the variance of the outlier scores as a possible measure for the signal power of the respective score. But the Soft-Min function operates on different scales for different values of gamma which makes a comparison hard. Just because a different scale/variance is used, this does not have to implicate that the score is better or worse. Instead, we are interested in the resulting ranking. That's why we will focus on the spearman rank correlation as well as our \"artificial\" accuracy specified above in the Hard-Min bootstrap case.\n",
    "\n",
    "Note that we tried different approaches, e.g. leaving the factor of 1/gamma out of the Soft-Min function to get comparable scales. We will also introduce some visual inspections. For example, in the following we can see how the Soft-Min score for a given instance changes for different values of gamma. We will use this for each instance to evaluate the development of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 338 has had a high outlier score\n",
    "instance = 338\n",
    "gammas = np.linspace(0.1, 2, 100)\n",
    "g = sns.lineplot(\n",
    "    x=gammas, y=[softmin(distances[instance, 1:], gamma) for gamma in gammas]\n",
    ")\n",
    "g.set_xlabel(\"gamma\")\n",
    "g.set_ylabel(\"softmin\") \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see how the scale of the Soft-Min score changes!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap/Robustness evaluation & the curse of choosing a bandwidth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Soft-Min score can be interpreted as a KDE-approach, the chosen gamma corresponds to the inverse of the bandwidth or variance of the used Gaussian distributions. Because of that, a high gamma leads to less biased estimates but with the cost of introducing a higher estimator variance! It is known that finding an appropriate bandwidth is a crucial and non-trivial task (and also way more important than choosing a kernel).\n",
    "\n",
    "Consequently, if we want to choose gamma, we have to evaluate the bias and variance of our estimator. To do this, we will use the bootstrap approach without resampling on a fraction size of the whole dataset. This will be set as a Hyperparameter and has to be chosen in a way that the data is varying enough but still carries enough information/is representative for the whole dataset. To evaluate the model variance for a given gamma, we will compute the score for each instance for each bootstrap round, then compute the within-variance of each sample across the different bootstrap rounds and finally aggregate the variances for each sample with a mean across the whole dataset.\n",
    "\n",
    "To analyse the bias, we will use the approaches discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmin(z, gamma):\n",
    "    return - (1/gamma)  * np.log(1 / (len(z) - 1) * np.sum(np.exp(-gamma * z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BOOTSTRAP = 100\n",
    "gamma_range = np.linspace(0.1, 2, 20)\n",
    "\n",
    "scores = np.full((len(df), N_BOOTSTRAP, len(gamma_range)), np.nan) # customer x bootstrap round x gamma\n",
    "\n",
    "for i in range(0, N_BOOTSTRAP):\n",
    "\n",
    "    sample = samples[i]\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=len(sample), algorithm=\"ball_tree\").fit(sample)\n",
    "    distances, indices = nbrs.kneighbors(sample)\n",
    "    distances = np.square(distances[:, 1:])\n",
    "    \n",
    "    for j, gamma in enumerate(gamma_range):\n",
    "        scores[sample.index, i, j] = np.apply_along_axis(softmin, 1, distances, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify normal distribution arround N_BOOTSTRAP * frac\n",
    "# sns.histplot(np.sum(np.isnan(scores) == False, axis=1)[:,0]).set_xlabel(\"# not sampled\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(scores, axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average outlier score per sample, per gamma\n",
    "avg_score = np.nanmean(scores, axis=1)\n",
    "\n",
    "display(avg_score.shape)\n",
    "print(\"average outlier score of instance 0:\")\n",
    "display(avg_score[0])\n",
    "\n",
    "print(\"average outlier score of instance 338:\")\n",
    "display(avg_score[338])\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "for i in range(len(scores)):\n",
    "    plt.plot(gamma_range, avg_score[i], linewidth=0.3)\n",
    "    plt.xlabel(\"Gamma\")\n",
    "    plt.ylabel(\"Soft-Min Score (mean)\")\n",
    "    plt.title(f\"Mean Soft-min Scores over {N_BOOTSTRAP} bootstraps for each customer\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the Soft-Min Scores are getting lower for all the samples for increasing gammas, the ranking more or less stays the same! Especially the ones with the highest scores are still just as separable. Hence, we discarded the variance over the scores as a potential measure of bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the spread over bootstrap dimension aka within sample variance\n",
    "spread = np.nanvar(scores, axis=1)\n",
    "\n",
    "# Comparing inlier vs outlier\n",
    "display(spread.shape)\n",
    "print(\"Vars of instance 0:\")\n",
    "display(spread[0])\n",
    "\n",
    "print(\"Vars of instance 338:\")\n",
    "display(spread[338])\n",
    "\n",
    "\n",
    "# all spreads per instances\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "for i in range(len(scores)):\n",
    "    plt.plot(gamma_range, spread[i], linewidth=0.3)\n",
    "    plt.xlabel(\"Gamma\")\n",
    "    plt.ylabel(\"Var(Soft-Min Score)\")\n",
    "    plt.title(f\"Variance of Soft-Min Scores over {N_BOOTSTRAP} bootstraps for each customer\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the variance of the model increases with increasing gamma just as expected. What strikes out is that it increases way more for outliers than for inliers. Since our purpose is to detect outliers, we have to carefully choose our gamma value to have robust estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check mean behaviour as well\n",
    "g = sns.scatterplot(x=gamma_range, y=np.mean(np.nanvar(scores, axis=1), axis=0))\n",
    "g.set_xlabel('Gamma')\n",
    "g.set_ylabel('Mean(Var(Soft-Min Score))')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the variance is way higher for the outliers, the mean reflects the model variance on a low scale as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(avg_score.shape)\n",
    "spr, acc = score_bias_eval(scores=avg_score[:, 1])\n",
    "print(f\"Bootstrap Accuracy: {acc:.2}% \\nSpearman results: {spr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_var_of_most_anomalous(customer_variances, n_most_outliers_considerd = OUTLIERS_FRAC):\n",
    "    n_most_outliers_considerd = math.ceil(len(customer_variances) * OUTLIERS_FRAC)\n",
    "    var_of_outliers = np.sort(customer_variances)[::-1][:n_most_outliers_considerd]\n",
    "    return np.mean(var_of_outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_measures = np.apply_along_axis(score_bias_eval, 0, avg_score)\n",
    "var_measures = np.apply_along_axis(mean_var_of_most_anomalous, 0, spread)\n",
    "gamma_overview = pd.DataFrame(bias_measures, index=[\"Spearman Corr\", \"Accuracy\"])\n",
    "gamma_overview.loc[\"Model Var\", :] = var_measures\n",
    "gamma_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Requires a motivation of choosing just the outlier points for computing model variance (relevant for the next section)\n",
    "\n",
    "g = sns.scatterplot(x=gamma_range, y=var_measures)\n",
    "g.set_xlabel('Gamma')\n",
    "g.set_ylabel('Model Variance')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the right gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axs = plt.subplots(1,2, figsize=(15, 5))\n",
    "\n",
    "g = sns.scatterplot(y=gamma_overview.loc[\"Spearman Corr\", ], x=gamma_overview.loc[\"Model Var\", ], hue=gamma_range, ax=axs[0])\n",
    "g.set_xlabel('Model Variance')\n",
    "g.set_ylabel('Spearman Correlation')\n",
    "\n",
    "g = sns.scatterplot(y=gamma_overview.loc[\"Accuracy\", ], x=gamma_overview.loc[\"Model Var\", ], hue=gamma_range, ax=axs[1])\n",
    "g.set_xlabel('Model Variance')\n",
    "g.set_ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that with increasing gamma, the model robustness as well its accuracy decreases. We have to choose a gamma which achieves a fair Trade-Off.\n",
    "\n",
    "Arguably, the gamma value should be at least above 0.3. Also, a Spearman Correlation of .9 is already quite high while the curve is getting very steep when the Spearman Correlation increases. The corresponding gamma value should therefore not exceed 1.2. For further analysis we take the highest value which has a relationship with model varoiance that could be approximated by a line.\n",
    "\n",
    "This is leeds to the Gamma value of 1.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could also restrict the spearman correlation on the most probable outliers just like in the case of the model variance. But since we also use the \"artificial\" accuracy, we can see that the Score is apropiate even in the \"crucial\" area of interest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Explaining Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance(x, yj, gamma):\n",
    "    \"\"\"Calculate layer-wise relevance propagation\n",
    "    @x: array of instances\n",
    "    @yj: softmin scores of all j instances\n",
    "    @gamma: gamma used for softmin \n",
    "    \"\"\"\n",
    "\n",
    "    Rji = np.zeros(x.shape)\n",
    "\n",
    "    # calculation per instance\n",
    "    for j in range(len(x)):\n",
    "\n",
    "        # mask is used to exclude the current instance j\n",
    "        mask = np.full((len(x)), True)\n",
    "        mask[j] = False\n",
    "\n",
    "        # save xk - xj\n",
    "        xk_j = x - x[j]\n",
    "\n",
    "        # calculate zk = ||xj - xk||^2\n",
    "        zk = np.square(np.linalg.norm(xk_j, axis=1))\n",
    "\n",
    "        # 1. First, one identifies to what extent each data point has contributed to the anomaly score of instance j\n",
    "        temp = np.exp(-gamma * zk[mask])\n",
    "        Rk = temp / np.sum(temp) * yj[j]\n",
    "\n",
    "        # 2. Then, these scores can be propagated back to the input features by observing that the (squared)\n",
    "        # Euclidean distance entering the anomaly score can be decomposed in terms of individual components:\n",
    "        Rji[j, :] = np.sum(np.square(xk_j)[mask] / zk[mask][:, None] * Rk[:, None], axis=0)\n",
    "\n",
    "    return Rji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1\n",
    "\n",
    "# Calculate anomaly scores\n",
    "nbrs = NearestNeighbors(n_neighbors=len(df), algorithm=\"ball_tree\").fit(df)\n",
    "distances, _ = nbrs.kneighbors(df)\n",
    "yj = np.apply_along_axis(softmin_og, 1, np.square(distances[:, 1:]), gamma)\n",
    "\n",
    "x = df.to_numpy()\n",
    "Rji = relevance(x, yj, gamma)\n",
    "\n",
    "# confirm conservation property\n",
    "np.all(np.sum(Rji, axis=1) - yj <= 1e-14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2, learning_rate='auto', init='pca', perplexity=30).fit_transform(df)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10,8))\n",
    "points = ax.scatter(x=X_embedded[:,1], y=X_embedded[:,0], c=yj, s=30, cmap=plt.cm.get_cmap('viridis_r'))\n",
    "f.colorbar(points)\n",
    "ax.set_ylabel(\"t-SNE 1\")\n",
    "ax.set_xlabel(\"t-SNE 2\")\n",
    "ax.set_title(\"Outlier score in 2D projection (t-SNE) \")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = 338\n",
    "g = sns.barplot(x=Rji[instance], y=list(map(str, list(zip(df_og.iloc[instance].index, df_og.iloc[instance].values)))))\n",
    "g.set_xlim(0,np.max(Rji))\n",
    "plt.show()\n",
    "\n",
    "# TODO WHY IS Fresh = 3 not relevant for outlier score?\n",
    "print(\"Ri:\", Rji[instance])\n",
    "print(\"Outlier score:\", yj[instance])\n",
    "display(df.iloc[instance])\n",
    "display(df.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment to use explanations of these outlier scores for the reproducibility experiment in task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50% random sample without replacement\n",
    "# for each gamma:\n",
    "#   calculate anomaly scores for sample instances\n",
    "#   calculate relevance \n",
    "#   save relevance (per component) for later analysis\n",
    "\n",
    "gamma = 1\n",
    "\n",
    "N_BOOTSTRAP = 100\n",
    "\n",
    "Rji_BS = np.full((440, N_BOOTSTRAP, len(gamma_range), 6), np.nan)\n",
    "\n",
    "for i in range(N_BOOTSTRAP):\n",
    "\n",
    "    sample = df.sample(frac=0.5)\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=len(sample), algorithm=\"ball_tree\").fit(sample)\n",
    "    distances, _ = nbrs.kneighbors(sample)\n",
    "    distances = np.square(distances[:, 1:])\n",
    "\n",
    "    for k, g in enumerate(gamma_range):\n",
    "        yj = np.apply_along_axis(softmin_og, 1, distances, g)\n",
    "        Rji_BS[sample.index, i, k, :] = relevance(sample.to_numpy(), yj, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each instance, for each sample, for each gamma, the relevance per component\n",
    "Rji_BS.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment for all components at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse the mean Euclidean distance between the relevance components over all bootstrap samples (per gamma and per instance)\n",
    "# since the range of the anomaly scores differes with gamma, the range of the relevance components will also depend on gamma\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "mean_dist = np.zeros((len(Rji_BS), len(gamma_range)))\n",
    "var_dist = np.zeros((len(Rji_BS), len(gamma_range)))\n",
    "\n",
    "# iterate over instances\n",
    "for j in range(len(Rji_BS)):\n",
    "    # iterate over gammas\n",
    "    for k in range(len(gamma_range)):\n",
    "        mean_dist[j,k] = np.nanmean(distance.pdist(Rji_BS[j,:,k,:]))\n",
    "        var_dist[j,k] = np.nanvar(distance.pdist(Rji_BS[j,:,k,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axs = axs.flatten()\n",
    "\n",
    "\n",
    "for i in range(len(var_dist)):\n",
    "    axs[0].plot(gamma_range, var_dist[i], linewidth=0.3)\n",
    "\n",
    "sns.scatterplot(x=gamma_range, y=np.mean(var_dist, axis=0), ax=axs[2])\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(mean_dist)):\n",
    "    axs[1].plot(gamma_range, mean_dist[i], linewidth=0.3)\n",
    "\n",
    "sns.scatterplot(x=gamma_range, y=np.var(mean_dist, axis=0), ax=axs[3])\n",
    "\n",
    "\n",
    "axs[0].set_ylabel(\"instance variance of component distance\")\n",
    "axs[1].set_ylabel(\"instance mean of component distance\")\n",
    "axs[2].set_ylabel(\"mean of instance variance of component distance\")\n",
    "axs[3].set_ylabel(\"variance of instance mean of component distance\")\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"gamma\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment PER COMPONENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component = 0\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axs = axs.flatten()\n",
    "\n",
    "\n",
    "spread = np.nanvar(Rji_BS[:, :, :, component], axis=1)\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    axs[0].plot(gamma_range, spread[i], linewidth=0.3)\n",
    "\n",
    "sns.scatterplot(x=gamma_range, y=np.mean(spread, axis=0), ax=axs[2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "avg_rel = np.nanmean(Rji_BS[:, :, :, component], axis=1)\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    axs[1].plot(gamma_range, avg_rel[i], linewidth=0.3)\n",
    "\n",
    "sns.scatterplot(x=gamma_range, y=np.var(avg_rel, axis=0), ax=axs[3])\n",
    "\n",
    "\n",
    "axs[0].set_ylabel(\"instance variance of relevance\")\n",
    "axs[1].set_ylabel(\"instance mean of relevance\")\n",
    "axs[2].set_ylabel(\"mean of instance variance of relevance\")\n",
    "axs[3].set_ylabel(\"variance of instance mean of relevance\")\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"gamma\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO do these findings match with our other findings?\n",
    "# TODO the components behave differently - why?\n",
    "# TODO remove outliers!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Cluster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Observation:\n",
    "Since the data does not appear to have natural cluster formations, and DBSCAN algorithm was not finding any bigger clusters, we decided to use K-means clustering algorithm. This was we can partition wholesale cusptmers into groups of relatively similar size that share some tendencies in their spending."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Selecting K parameter for K-means\n",
    "\n",
    "Compute visualisation to identify the optimal inflection point for K in [2, 15]:\n",
    "* elbow (distortion score): the sum of squared distances from each point to its assigned center:\n",
    "* silhouette score: the mean Silhouette Coefficient of all samples;\n",
    "* Calinski-Harabasz score: the ratio of dispersion between and within clusters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# K-means initialised with greedy k-means++ algorithm, over 100 initialisations\n",
    "kmeans = KMeans(init=\"k-means++\", n_init=100, random_state=42)\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "gs = gridspec.GridSpec(2,3)\n",
    "ax = {}\n",
    "\n",
    "for i, metr in zip(range(3), ['distortion', 'silhouette', 'calinski_harabasz']):\n",
    "    ax[i] = fig.add_subplot(gs[i])\n",
    "    ax[i] = visualizer = KElbowVisualizer(kmeans, k=(2,16), metric=metr, timings=False, show=True)\n",
    "    visualizer.fit(df)\n",
    "    visualizer.finalize()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Observation:\n",
    "There is no clear infliction point in the elbow plot of the distoriotn score, we can assume K = 6 to be a reasonable trade-off point. Similar result is obtained by computing the Calinski-Harabasz index.\n",
    "The silhouette score is strongly reduced for K > 3, however we assume that more than three clusters is reasonable in order to partition the customers in this application.\n",
    "\n",
    "#### In addition we perform K-means clustering for K in the range [2, 6] and inspect the silhouette scores for the clusters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p1_functions.silhouette_analysis(min_k=2, max_k=8, X=df, Umap=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Observation:\n",
    "It seems that the most plausible solution for K > 3 is K = 5 or K = 6. The silhouette scores are overall not very satisfying, but they are a bit higher for these two clusterings. As for individual clusters, the clustering with K = 6 has a small cluster with index 0, which in not having a high score, but seems to allow reorganisation and slightly better scoring for the other clusters. Both these options could be reasonable and would need to be discussed in the context of the application. Since we do not have this context, we fix K to six as this number of clusters also provides a lower distortion score."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fit k-Means and visualise using UMAP and t-SNE\n",
    "K = 6\n",
    "kmeans = KMeans(n_clusters=K, init=\"k-means++\", n_init=100, random_state=42).fit(df)\n",
    "p1_functions.visualise_kmeans(df, kmeans)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"k-Means cluster sizes:\")\n",
    "for cluster in set(kmeans.labels_):\n",
    "    filter = kmeans.labels_ == cluster\n",
    "    print(\"Cluster:\", cluster, \":\", sum(kmeans.labels_ == cluster))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Interpretation of the clustering\n",
    "For an interpretation of the clustering, we plot average spending of cluster members for the different product categories.\n",
    "We computed means and standard deviations for each category within each cluster, as well as boxplots for each clsuter within each category."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p1_functions.clusters_stats(df, kmeans)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-2.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
